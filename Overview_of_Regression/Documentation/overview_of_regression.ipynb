{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Overview_of_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "216e333e888904ba5b259a6fd107b056d501c561085a2f0a97f72454058bd35b"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#REGRESSION\r\n",
        "\r\n",
        "**Regression analysis** is a fundamental concept in the field of machine learning. It falls under supervised learning wherein the algorithm is trained with both input features and output labels. It helps in establishing a relationship among the variables by estimating how one variable affects the other.\r\n",
        "\r\n",
        "\r\n",
        "Example: Suppose there is a marketing company A, who does various advertisement every year and get sales on that. The below list shows the advertisement made by the company in the last 5 years and the corresponding sales:\r\n",
        "\r\n",
        "\r\n",
        "![image](images\\regression-analysis-in-machine-learning.png)\r\n",
        "\r\n",
        "Now, the company wants to do the advertisement of $200 in the year 2019 and wants to know the prediction about the sales for this year. So to solve such type of prediction problems in machine learning, we need regression analysis."
      ],
      "metadata": {
        "id": "EDitLoe_iWks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression** is a supervised learning technique which helps in finding the correlation between variables and enables us to predict the continuous output variable based on the one or more predictor variables. It is mainly used for prediction, forecasting, time series modeling, and determining the causal-effect relationship between variables.\r\n",
        "\r\n",
        "In Regression, we plot a graph between the variables which best fits the given datapoints, using this plot, the machine learning model can make predictions about the data. In simple words, \"Regression shows a line or curve that passes through all the datapoints on target-predictor graph in such a way that the vertical distance between the datapoints and the regression line is minimum.\" The distance between datapoints and line tells whether a model has captured a strong relationship or not.\r\n",
        "\r\n",
        "Some examples of regression can be as:\r\n",
        "\r\n",
        "- Prediction of rain using temperature and other factors\r\n",
        "\r\n",
        "- Determining Market trends\r\n",
        "\r\n",
        "- Prediction of road accidents due to rash driving.\r\n",
        "\r\n",
        "##Evaluating a Regression\r\n",
        "Let’s say you’ve developed an algorithm which predicts next week's temperature. The temperature to be predicted depends on different properties such as humidity, atmospheric pressure, air temperature and wind speed. But how accurate are your predictions? How good is your algorithm?\r\n",
        "\r\n",
        "To evaluate your predictions, there are two important metrics to be considered: variance and bias.\r\n",
        "\r\n",
        "###Variance\r\n",
        "\r\n",
        "Variance is the amount by which the estimate of the target function changes if different training data were used. The target function $f$ establishes the relation between the input (properties) and the output variables (predicted temperature). When a different dataset is used the target function needs to remain stable with little variance because, for any given type of data, the model should be generic. In this case, the predicted temperature changes based on the variations in the training dataset. To avoid false predictions, we need to make sure the variance is low. For that reason, the model should be generalized to accept unseen features of temperature data and produce better predictions.\r\n",
        "\r\n",
        "###Bias\r\n",
        "\r\n",
        "Bias is the algorithm’s tendency to consistently learn the wrong thing by not taking into account all the information in the data. For the model to be accurate, bias needs to be low. If there are inconsistencies in the dataset like missing values, less number of data tuples or errors in the input data, the bias will be high and the predicted temperature will be wrong.\r\n",
        "\r\n",
        "For a model to be ideal, it’s expected to have low variance, low bias and low error. To achieve this, we need to partition the dataset into train and test datasets. The model will then learn patterns from the training dataset and the performance will be evaluated on the test dataset. If the model memorizes/mimics the training data fed to it, rather than finding patterns, it will give false predictions on unseen data. The curve derived from the trained model would then pass through all the data points and the accuracy on the test dataset is low. This is called overfitting and is caused by high variance. \r\n",
        "On the flip side, if the model performs well on the test data but with low accuracy on the training data, then this leads to underfitting.\r\n",
        "\r\n",
        "![bias](images/underfitting.png)\r\n",
        "\r\n",
        "\r\n",
        "There are various algorithms that are used to build a regression model, some work well under certain constraints and some don’t.\r\n"
      ],
      "metadata": {
        "id": "govNeXfuh9XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LINEAR REGRESSION\r\n",
        "\r\n",
        "Linear regression finds the linear relationship between the dependent variable and one or more independent variables using a best-fit straight line. Generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term). In this technique, the dependent variable is continuous, the independent variable(s) can be continuous or discrete, and the nature of the regression line is linear. Mathematically, the prediction using linear regression is given as:\r\n",
        "\r\n",
        "$$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + … + \\theta_nx_n$$\r\n",
        "\r\n",
        "Here, $y$ is the predicted value,\r\n",
        "\r\n",
        "$n$ is the total number of input features,\r\n",
        "\r\n",
        "$x_i$ is the input feature for $i^{th}$ value, \r\n",
        "\r\n",
        "$\\theta_i$ is the model parameter ($\\theta_0$ is the bias and the coefficients are $\\theta_1, \\theta_2, … \\theta_n$).\r\n",
        "\r\n",
        "The coefficient is like a volume knob, it varies according to the corresponding input attribute, which brings change in the final value. It signifies the contribution of the input variables in determining the best-fit line. \r\n",
        "\r\n",
        "Bias is a deviation induced to the line equation $y = mx$ for the predictions we make. We need to tune the bias to vary the position of the line that can fit best for the given data. \r\n",
        "\r\n",
        "![linreg](images\\linear.jpeg)\r\n",
        "\r\n",
        "- Linear regression shows the linear relationship between the independent variable (X-axis) and the dependent variable (Y-axis), hence called linear regression.\r\n",
        "- If there is only one input variable (x), then such linear regression is called simple linear regression. And if there is more than one input variable, then such linear regression is called multiple linear regression.\r\n",
        "- The relationship between variables in the linear regression model can be explained using the below image. Here we are predicting the salary of an employee on the basis of the year of experience."
      ],
      "metadata": {
        "id": "WvRDtI_Mtu_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Least Squares Method\r\n",
        "\r\n",
        "First, calculate the error/loss by subtracting the actual value from the predicted one. Since the predicted values can be on either side of the line, we square the difference to make it a positive value. The result is denoted by ‘Q’, which is known as the sum of squared errors.\r\n",
        "\r\n",
        "Mathematically:\r\n",
        "\r\n",
        "$$Q =\\sum_{i=1}^{n}(y_{predicted}-y_{original} )^2$$\r\n",
        "\r\n",
        "Our goal is to minimize the error function ‘Q.\" To get to that, we differentiate Q w.r.t ‘m’ and ‘c’ and equate it to zero. After a few mathematical derivations  ‘m’ will be \r\n",
        "\r\n",
        "$$m = \\frac{cov(x,y)}{var(x)}$$ \r\n",
        "\r\n",
        "And ‘c’ will be,\r\n",
        "\r\n",
        "$$c = y^{-} - bx^{-}$$\r\n",
        "\r\n",
        "By plugging the above values into the linear equation, we get the best-fit line.\r\n"
      ],
      "metadata": {
        "id": "U-mxrOeDtcoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Descent\r\n",
        "\r\n",
        "Gradient descent is an optimization technique used to tune the coefficient and bias of a linear equation.\r\n",
        "\r\n",
        "Imagine you are on the top left of a u-shaped cliff and moving blind-folded towards the bottom center. You take small steps in the direction of the steepest slope. This is what gradient descent does — it is the derivative or the tangential line to a function that attempts to find local minima of a function.\r\n",
        "We take steps down the cost function in the direction of the steepest descent until we reach the minima, which in this case is the downhill. The size of each step is determined by the parameter $\\alpha$, called learning rate. If it's too big, the model might miss the local minimum of the function, and if it's too small, the model will take a long time to converge. Hence, $\\alpha$ provides the basis for finding the local minimum, which helps in finding the minimized cost function.\r\n",
        "\r\n",
        "‘Q’ the cost function is differentiated w.r.t the parameters, $m$ and $c$ to arrive at the updated $m$ and $c$, respectively. The product of the differentiated value and learning rate is subtracted from the actual ones to minimize the parameters affecting the model.\r\n",
        "\r\n",
        "Mathematically, this is how parameters are updated using the gradient descent algorithm:\r\n",
        "\r\n",
        "$$m = m - \\alpha\\frac{d}{dm}Q$$ \r\n",
        "\r\n",
        "$$c = c - \\alpha\\frac{d}{dc}Q$$ \r\n",
        "\r\n",
        "where $Q =\\sum_{i=1}^{n}(y_{predicted}-y_{original} )^2$.\r\n",
        "\r\n",
        "This continues until the error is minimized."
      ],
      "metadata": {
        "id": "E8IZ9FbqMj7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example\r\n",
        "\r\n",
        "The example below uses only the first feature of the diabetes dataset, in order to illustrate the data points within the two-dimensional plot. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.\r\n",
        "\r\n",
        "The coefficients, residual sum of squares and the coefficient of determination are also calculated.\r\n",
        "The library used for linear regression is **sklearn**."
      ],
      "metadata": {
        "id": "2tGOVPy_M8Ew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "print(__doc__)\r\n",
        "\r\n",
        "\r\n",
        "# Code source: Jaques Grobler\r\n",
        "# License: BSD 3 clause\r\n",
        "\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from sklearn import datasets, linear_model\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score\r\n",
        "\r\n",
        "# Load the diabetes dataset\r\n",
        "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\r\n",
        "\r\n",
        "# Use only one feature\r\n",
        "diabetes_X = diabetes_X[:, np.newaxis, 2]\r\n",
        "\r\n",
        "# Split the data into training/testing sets\r\n",
        "diabetes_X_train = diabetes_X[:-20]\r\n",
        "diabetes_X_test = diabetes_X[-20:]\r\n",
        "\r\n",
        "# Split the targets into training/testing sets\r\n",
        "diabetes_y_train = diabetes_y[:-20]\r\n",
        "diabetes_y_test = diabetes_y[-20:]\r\n",
        "\r\n",
        "# Create linear regression object\r\n",
        "regr = linear_model.LinearRegression()\r\n",
        "\r\n",
        "# Train the model using the training sets\r\n",
        "regr.fit(diabetes_X_train, diabetes_y_train)\r\n",
        "\r\n",
        "# Make predictions using the testing set\r\n",
        "diabetes_y_pred = regr.predict(diabetes_X_test)\r\n",
        "\r\n",
        "# The coefficients\r\n",
        "print('Coefficients: \\n', regr.coef_)\r\n",
        "# The mean squared error\r\n",
        "print('Mean squared error: %.2f'\r\n",
        "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\r\n",
        "# The coefficient of determination: 1 is perfect prediction\r\n",
        "print('Coefficient of determination: %.2f'\r\n",
        "      % r2_score(diabetes_y_test, diabetes_y_pred))\r\n",
        "\r\n",
        "# Plot outputs\r\n",
        "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\r\n",
        "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\r\n",
        "\r\n",
        "plt.xticks(())\r\n",
        "plt.yticks(())\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "Coefficients: \n",
            " [938.23786125]\n",
            "Mean squared error: 2548.07\n",
            "Coefficient of determination: 0.47\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "5KJth-4WNpPc",
        "outputId": "20245ecc-4659-4b16-b724-3d578c8528e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POLYNOMIAL REGRESSION\r\n",
        "\r\n",
        "Polynomial regression is a special case of linear regression where we fit a polynomial equation on the data with a curvilinear relationship between the target variable and the independent variables.\r\n",
        "- It is similar to multiple linear regression, but it fits a non-linear curve between the value of x and corresponding conditional values of y.\r\n",
        "- Suppose there is a dataset which consists of datapoints which are present in a non-linear fashion, so for such case, linear regression will not best fit to those datapoints. To cover such datapoints, we need Polynomial regression.\r\n",
        "- In Polynomial regression, the original features are transformed into polynomial features of given degree and then modeled using a linear model. Which means the datapoints are best fitted using a polynomial line.\r\n",
        "\r\n",
        "$$Y_{0} = b_{0}+ b_{1}x^{1} + … b_{n}x^{n}$$\r\n",
        "\r\n",
        "where $Y_{0}$ is the predicted value for the polynomial model with regression coefficients $b_{1}$ to $b_{n}$ for each degree and a bias of $b_{0}$.\r\n",
        "\r\n",
        "If n=1, the polynomial equation is said to be a linear equation.\r\n",
        "\r\n",
        "![pol](images\\pr7.png)"
      ],
      "metadata": {
        "id": "PtRSHlcqVFYC"
      }
    }
  ]
}